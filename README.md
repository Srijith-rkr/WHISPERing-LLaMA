# WHISPERing-LLaMA: Integrate Whisper Encoder to LLaMA Decoder

- Accepted at **EMNLP 2023 (Main Track)**
- ASR Generative Error Correction by leveraging foundational Audio (Whisper) and Langugae (LLaMA) models.

<p align="center">  <img src="https://github.com/Srijith-rkr/WHISPERing-LLaMA/blob/main/images/model%20overview.svg" height ="450"> </p>

# Introduction 
<p align="center">  <img src="https://github.com/Srijith-rkr/WHISPERing-LLaMA/blob/main/images/Prompt%20overview.svg" height ="450"> </p>
<p align="center">  <img src="https://github.com/Srijith-rkr/WHISPERing-LLaMA/blob/main/images/Mechanism%20overview.svg" width="700"> </p>
# Fine-tune and Run Inference

# Setup

# Training & Inference

# Acknowledgements

This implementation builds on 
- [lit-llama](https://github.com/Lightning-AI/lit-llama) for the Training pipeline.
- [stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca) for the pre-trained instruction following Language model.
- [Whisper](https://github.com/openai/whisper) to obtain acoustic embeddings.
